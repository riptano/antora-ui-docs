= Using Apache Spark to connect your database
:slug: connect-the-spark-cassandra-connector-to-astra

Use Apache Spark to connect to your database and begin accessing your {astra_db} tables using Scala in spark-shell. Connect Spark to {astra_db}, run SQL statements, interact with Spark DataFrames/RDDs, or even run CQL statements directly.

== Prerequisites
. Click **Download Bundle** at the top of any language (Connect using a driver or Spark under Integrate with other tools) page for connection credentials to your database. For more, see xref:connect:secure-connect-bundle.adoc#_downloading_secure_connect_bundle[Downloading secure connect bundle].

+
image::download-bundle1.png[Download bundle,1900,2000]

. Download https://spark.apache.org/downloads.html[Apache Spark] pre-built for Apache Hadoop 2.7.
. Create a xref:getting-started:gs-grant-user-access.adoc[Application Token] with the appropriate role set (RO User is needed for example below). 
. Download the https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector[Spark Cassandra Connector] (SCC) that matches your Apache Spark and Scala version from the maven central repository. To find the right version of SCC, check the https://github.com/datastax/spark-cassandra-connector#version-compatibility[SCC compatibility].

== Procedure

[NOTE]
//
Use the following steps if you are using Apache Spark in local mode. 
//

. Expand the downloaded Apache Spark package into a directory and assign the directory name to \$SPARK_HOME (cd \$SPARK_HOME). 
. Append the following lines at the end of a file, $SPARK_HOME/conf/spark-defaults.conf. If necessary, look for a template under the $SPARK_HOME/conf directory. 
. Replace the second column (value) with the first four lines: 

```text
spark.files $SECURE_CONNECT_BUNDLE_FILE_PATH/secure-connect-{{safeName}}.zip
spark.cassandra.connection.config.cloud.path secure-connect-{{safeName}}.zip
spark.cassandra.auth.username <<CLIENT ID>>
spark.cassandra.auth.password <<CLIENT SECRET>>
spark.dse.continuousPagingEnabled false
```
[start=4]
. Launch spark-shell and enter the following scala commands:

```scala
import com.datastax.spark.connector._
import org.apache.spark.sql.cassandra._
spark.read.cassandraFormat("tables", "system_schema").load().count()
```

The following output appears:

```text
$ bin/spark-shell
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1608781805157).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.9.1)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import com.datastax.spark.connector._
import com.datastax.spark.connector._

scala> import org.apache.spark.sql.cassandra._
import org.apache.spark.sql.cassandra._

scala> spark.read.cassandraFormat("tables", "system_schema").load().count()
res0: Long = 25

scala> :quit
```

The Spark Cassandra Connector (SCC) is available for any Cassandra user, including Astra users. The SCC allows for better support of container orchestration platforms. For more, read https://www.datastax.com/blog/2020/05/advanced-apache-cassandra-analytics-now-open-all[Advanced Apache Cassandra Analytics Now Open for All].

//// 
Connect Spark to Cassandra and execute CQL statements from Spark applications.

The Spark Cassandra Connector (SCC) 2.5 is available for any Cassandra user, including Astra users.
The SCC allows for better support of container orchestration platforms.
For more, read https://www.datastax.com/blog/2020/05/advanced-apache-cassandra-analytics-now-open-all[Advanced Apache Cassandra Analytics Now Open for All].

== Preparing `SparkContext` to work with Cassandra

Set connection options in `$SPARK_HOME/conf/spark-default.conf`.
These are prefixed with `spark.` so that they can be recognized from `spark-shell`.

=== Example

[source, scala]
----
val conf = new SparkConf(true)
        .set("spark.cassandra.connection.host", "192.168.123.10")
        .set("spark.cassandra.auth.username", "cassandra")
        .set("spark.cassandra.auth.password", "cassandra")

val sc = new SparkContext("spark://192.168.123.10:7077", "test", conf)
----

You can pass multiple hosts using a comma-separated list in `spark.cassandra.connection.host` (e.g. `"127.0.0.1,127.0.0.2"`).
These are only the initial contact points only.
All nodes in your local region will be used upon connecting.

For more, see the https://github.com/datastax/spark-cassandra-connector/blob/master/doc/reference.md#cassandra-connection-parameters[Cassandra Connection Parameters].

== Connecting using an Astra Cloud Bundle or Driver Profile File (Since SCC 2.5)

You can use a separate configuration file if the file is already accessible on a distributed file system (hdfs:// or s3a:// for example), is distributed by Spark, or is already on the Spark Classpath on Driver and Executor Nodes.

If your file needs to be distributed by Spark use the `--files` option in Spark Submit or `SparkContext.addFile`.
For these files, pass the file name to either of the following parameters without any other path info:

* `spark.cassandra.connection.config.cloud.path` for use with an xref:connect:secure-connect-bundle.adoc[Astra secure connect bundle].
This requires the Client ID, Client Secret, and corresponding configuration properties. For more, see xref:manage-application-token.adoc[Managing application tokens].
* `spark.cassandra.connection.config.profile.path` for use with a Java Driver https://docs.datastax.com/en/developer/java-driver/4.2/manual/core/configuration/[Profile].

[IMPORTANT]
====
When using a profile file, all other configuration will be ignored.
====

== Connection management

When you call a method requiring access to Cassandra, the options in the `SparkConf` object are used to create a new connection or to borrow one already open from the global connection cache.

=== Initial contact

The initial contact node given in `spark.cassandra.connection.host` can be any node of the cluster.
The driver fetches the cluster topology from the contact node and always tries to connect to the closest node in the same region.
If possible, connections are established to the same node the task is running on.
Consequently, good locality of data can be achieved and the amount of data sent across the network is minimized.

=== Inter-region communication is forbidden by default

Connections are never made to regions other than the region of `spark.cassandra.connection.host`.
If some nodes in the local region are down and a read or write operation fails, the operation won't be retried on nodes in a different region.
This technique guarantees proper workload isolation so that a huge analytics job won't disturb the real-time part of the system.

=== Connection Pooling

Connections are cached internally.
If you call two methods needing access to the same Cassandra cluster quickly, one after another, or in parallel from different threads, they will share the same logical connection represented by the underlying Java Driver `Cluster` object.

[source, scala]
----
val connector = CassandraConnector(sc.getConf)
connector.withSessionDo(session => ...)
connector.withSessionDo(session => ...)
----

or

[source, scala]
----
val connector = CassandraConnector(sc.getConf)
sc.parallelize(1 to 100).mapPartitions( it => connector.withSessionDo( session => ...))
----

This method will not use more than one `Cluster` object or `Session` object per JVM.

When all the tasks needing Cassandra connectivity terminate, the connection to the Cassandra cluster closes shortly thereafter.
The period of time for keeping unused connections open is controlled by the global `spark.cassandra.connection.keep_alive_ms` system property.
For more, see the https://github.com/datastax/spark-cassandra-connector/blob/master/doc/reference.md#cassandra-connection-parameters[Cassandra Connection Parameters].

== Connecting manually to Cassandra

If you need to manually connect to Cassandra to issue CQL statements, this driver offers a `CassandraConnector` class, which can be initialized from the `SparkConf` object and provides access to the `Cluster` and `Session` objects.
`CassandraConnector` instances are serializable and therefore can be safely used in lambdas passed to Spark transformations as seen in the examples above.

Assuming an appropriately configured `SparkConf` object is stored in the `conf` variable, the following code creates a keyspace and a table:

[source, scala]
----
import com.datastax.spark.connector.cql.CassandraConnector

CassandraConnector(conf).withSessionDo { session =>
  session.execute("CREATE KEYSPACE test2 WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 }")
  session.execute("CREATE TABLE test2.words (word text PRIMARY KEY, count int)")
}
----

== Connecting to multiple Cassandra Clusters

The Spark Cassandra Connector is able to connect to multiple Cassandra clusters for all of its normal operations.
The default `CassandraConnector` object used by calls to `sc.cassandraTable` and `saveToCassandra` is specified by the `SparkConfiguration`.
If you want to use multiple clusters, an implicit `CassandraConnector` can be used in a code block to specify the target cluster for all operations in that block.

=== Example of reading from one cluster and writing to another

[source, scala]
----
import com.datastax.spark.connector._
import com.datastax.spark.connector.cql._

import org.apache.spark.SparkContext

def twoClusterExample ( sc: SparkContext) = {
  val connectorToClusterOne = CassandraConnector(sc.getConf.set("spark.cassandra.connection.host", "127.0.0.1"))
  val connectorToClusterTwo = CassandraConnector(sc.getConf.set("spark.cassandra.connection.host", "127.0.0.2"))

  val rddFromClusterOne = {
    // Sets connectorToClusterOne as default connection for everything in this code block
    implicit val c = connectorToClusterOne
    sc.cassandraTable("ks","tab")
  }

  {
    //Sets connectorToClusterTwo as the default connection for everything in this code block
    implicit val c = connectorToClusterTwo
    rddFromClusterOne.saveToCassandra("ks","tab")
  }

}
----
////