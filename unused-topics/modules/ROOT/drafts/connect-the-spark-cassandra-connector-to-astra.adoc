= Connect the Spark Cassandra Connector to Astra
:slug: connect-the-spark-cassandra-connector-to-astra

Connect Spark to Cassandra and execute CQL statements from Spark applications.

The Spark Cassandra Connector (SCC) 2.5 is available for any Cassandra user, including Astra users.
The SCC allows for better support of container orchestration platforms.
For more, read https://www.datastax.com/blog/2020/05/advanced-apache-cassandra-analytics-now-open-all[Advanced Apache Cassandra Analytics Now Open for All].

== Preparing `SparkContext` to work with Cassandra

Set connection options in `$SPARK_HOME/conf/spark-default.conf`.
These are prefixed with `spark.` so that they can be recognized from `spark-shell`.

=== Example

[source, scala]
----
val conf = new SparkConf(true)
        .set("spark.cassandra.connection.host", "192.168.123.10")
        .set("spark.cassandra.auth.username", "cassandra")
        .set("spark.cassandra.auth.password", "cassandra")

val sc = new SparkContext("spark://192.168.123.10:7077", "test", conf)
----

You can pass multiple hosts using a comma-separated list in `spark.cassandra.connection.host` (e.g. `"127.0.0.1,127.0.0.2"`).
These are only the initial contact points only.
All nodes in your local region will be used upon connecting.

For more, see the https://github.com/datastax/spark-cassandra-connector/blob/master/doc/reference.md#cassandra-connection-parameters[Cassandra Connection Parameters].

== Connecting using an Astra Cloud Bundle or Driver Profile File (Since SCC 2.5)

You can use a separate configuration file if the file is already accessible on a distributed file system (hdfs:// or s3a:// for example), is distributed by Spark, or is already on the Spark Classpath on Driver and Executor Nodes.

If your file needs to be distributed by Spark use the `--files` option in Spark Submit or `SparkContext.addFile`.
For these files, pass the file name to either of the following parameters without any other path info:

* `spark.cassandra.connection.config.cloud.path` for use with an xref:connect:secure-connect-bundle.adoc[Astra secure connect bundle].
This requires the Client ID, Client Secret, and corresponding configuration properties. For more, see xref:manage-application-token.adoc[Managing application tokens].
* `spark.cassandra.connection.config.profile.path` for use with a Java Driver https://docs.datastax.com/en/developer/java-driver/4.2/manual/core/configuration/[Profile].

[IMPORTANT]
====
When using a profile file, all other configuration will be ignored.
====

== Connection management

When you call a method requiring access to Cassandra, the options in the `SparkConf` object are used to create a new connection or to borrow one already open from the global connection cache.

=== Initial contact

The initial contact node given in `spark.cassandra.connection.host` can be any node of the cluster.
The driver fetches the cluster topology from the contact node and always tries to connect to the closest node in the same region.
If possible, connections are established to the same node the task is running on.
Consequently, good locality of data can be achieved and the amount of data sent across the network is minimized.

=== Inter-region communication is forbidden by default

Connections are never made to regions other than the region of `spark.cassandra.connection.host`.
If some nodes in the local region are down and a read or write operation fails, the operation won't be retried on nodes in a different region.
This technique guarantees proper workload isolation so that a huge analytics job won't disturb the real-time part of the system.

=== Connection Pooling

Connections are cached internally.
If you call two methods needing access to the same Cassandra cluster quickly, one after another, or in parallel from different threads, they will share the same logical connection represented by the underlying Java Driver `Cluster` object.

[source, scala]
----
val connector = CassandraConnector(sc.getConf)
connector.withSessionDo(session => ...)
connector.withSessionDo(session => ...)
----

or

[source, scala]
----
val connector = CassandraConnector(sc.getConf)
sc.parallelize(1 to 100).mapPartitions( it => connector.withSessionDo( session => ...))
----

This method will not use more than one `Cluster` object or `Session` object per JVM.

When all the tasks needing Cassandra connectivity terminate, the connection to the Cassandra cluster will be closed shortly thereafter.
The period of time for keeping unused connections open is controlled by the global `spark.cassandra.connection.keep_alive_ms` system property.
For more, see the https://github.com/datastax/spark-cassandra-connector/blob/master/doc/reference.md#cassandra-connection-parameters[Cassandra Connection Parameters].

== Connecting manually to Cassandra

If you need to manually connect to Cassandra to issue CQL statements, this driver offers a `CassandraConnector` class, which can be initialized from the `SparkConf` object and provides access to the `Cluster` and `Session` objects.
`CassandraConnector` instances are serializable and therefore can be safely used in lambdas passed to Spark transformations as seen in the examples above.

Assuming an appropriately configured `SparkConf` object is stored in the `conf` variable, the following code creates a keyspace and a table:

[source, scala]
----
import com.datastax.spark.connector.cql.CassandraConnector

CassandraConnector(conf).withSessionDo { session =>
  session.execute("CREATE KEYSPACE test2 WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 }")
  session.execute("CREATE TABLE test2.words (word text PRIMARY KEY, count int)")
}
----

== Connecting to multiple Cassandra Clusters

The Spark Cassandra Connector is able to connect to multiple Cassandra clusters for all of its normal operations.
The default `CassandraConnector` object used by calls to `sc.cassandraTable` and `saveToCassandra` is specified by the `SparkConfiguration`.
If you want to use multiple clusters, an implicit `CassandraConnector` can be used in a code block to specify the target cluster for all operations in that block.

=== Example of reading from one cluster and writing to another

[source, scala]
----
import com.datastax.spark.connector._
import com.datastax.spark.connector.cql._

import org.apache.spark.SparkContext

def twoClusterExample ( sc: SparkContext) = {
  val connectorToClusterOne = CassandraConnector(sc.getConf.set("spark.cassandra.connection.host", "127.0.0.1"))
  val connectorToClusterTwo = CassandraConnector(sc.getConf.set("spark.cassandra.connection.host", "127.0.0.2"))

  val rddFromClusterOne = {
    // Sets connectorToClusterOne as default connection for everything in this code block
    implicit val c = connectorToClusterOne
    sc.cassandraTable("ks","tab")
  }

  {
    //Sets connectorToClusterTwo as the default connection for everything in this code block
    implicit val c = connectorToClusterTwo
    rddFromClusterOne.saveToCassandra("ks","tab")
  }

}
----
